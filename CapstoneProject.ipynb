{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Capstone Project: Systematic Trading Strategies\n",
    "\n",
    "## Goal: \n",
    "Implement a process of developing trading strategies in Indian Equity Markets using fundamental data of liquid listed companies that maximises out of sample average returns keeping risk under control.\n",
    "\n",
    "## Who cares?\n",
    "\n",
    "I do. I am planning to start my own investment firm. I have not focussed much on fundamental data before, neither have I explored advanced machine learning techniques to develop trading strategies. I specialize in analyzing short-term (seconds to minutes) movements in markets, not over months and years. If there indeed is value derived out of using some subset of Data Science that I learn during this course, I am happy to invest my own money on these trading strategies. Even otherwise, I believe making investment process data-driven and automated has benefits for everyone interested invest wisely and with minimum cost. \n",
    "\n",
    "## Data\n",
    "\n",
    "I have subscribed to two data sets from Quandl.\n",
    "\n",
    "1. [https://www.quandl.com/data/DEB-Core-India-Fundamentals-Data](https://www.quandl.com/data/DEB-Core-India-Fundamentals-Data) Contains fundamental indicators derived from company financials from published balance sheet, income statements and cash flow statements from roughly 4000 listed stocks on NSE or BSE.  History goes back to 2005\n",
    "2. [https://www.quandl.com/data/TC1-Indian-Equities-Adjusted-End-of-Day-Prices](https://www.quandl.com/data/TC1-Indian-Equities-Adjusted-End-of-Day-Prices) Contains daily prices (open, low, high, close), volume and value traded for constituents of top 500 NSE listed stocks by market capitalization. The prices are adjusted from corporate actions like stock splits, rights issues, dividends, buy backs etc.\n",
    "\n",
    "## Approach\n",
    "\n",
    "We wish to follow the approach of a white paper published by a Deutsche Bank research group. In that paper the authors review various machine learning algorithms and apply them in practice on Japanese Equity markets data. The Following are the steps involved\n",
    "\n",
    "1. **Data Selection** To decide the stock universe, date range for training set and test-set. Factors involved:\n",
    "  1. Data availability\n",
    "  2. Stock liquidity\n",
    "  3. Avoid any bias in universe selection like survivorship bias\n",
    "2. ** Problem Formulation: ** Describe \n",
    "    1. the context,\n",
    "    2. the feature space, \n",
    "    3. the target variable, \n",
    "    3. the optimization problem and \n",
    "    4. the model selection metric and \n",
    "    5. the evaluation criteria\n",
    "2. **Investment signal creation and classification**: A close look at fundamental data set to see whether it needs enrichment i.e. computing well known indicators to make a comprehensive list of possible investment signals classified into various factors like growth, value, quality, size, momentum etc\n",
    "3. **Visualize performance of individual investment signals ** If each investment signal is used only by itself as a ranker for long stock portfolio, how is the in-sample performance of the strategy.\n",
    "4. **Data pre-processing** like filling missing data, normalizing, uniformizing, sector-neutralizing, quantizing, winsorizing etc to make variations in output variable (returns) more sensitive to variations in input variables and also make combining variables more sensible.\n",
    "5. **Create a benchmark** by using a simple linear model (with some basic checks) for other machine learning techniques to see if they do any better\n",
    "6. **Test most promising models** as per the paper and see if they perform any better than the simple model\n",
    "7. **Document the conclusions and insights**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection \n",
    "To decide the date range for training set and test-set and stock universe for this study. Factors involved:\n",
    "\n",
    "1. Data availability: \n",
    "    1. Fundamental Data-set is available 2005 onwards. \n",
    "    2. Adjusted Price data is available 2001 onwards. \n",
    "    3. Earnings release dates are available 2011 onwards. \n",
    "    4. There are more than 1000 stocks common in all the three data-sets\n",
    "    5. ** Decisions: ** \n",
    "        1. Lets *not* throw away 2005-10 for absence of earnings release dates. Instead use it with a delay of 63 days from quarter end whenever earnings release dates are not available\n",
    "        2. Training set: 2005 to 2011 \n",
    "        3. Test set: 2012 - 2015 \n",
    "        4. Reserved set (only for final reporting): 2016 - 2017 \n",
    "2. Stock liquidity:\n",
    "    1. Small cap stocks are known to be prone to manipulation (both of stock market and company fundamentals) \n",
    "    2. Trading in large amounts in smaller stocks will lead to heavy transaction costs. \n",
    "    3. ** Decision: ** Let us use top 200 stocks by market cap at each rebalance date. \n",
    "3. Avoid any bias in universe selection like survivorship:\n",
    "    1. ** Decision: ** Instead of a fixed universe of stocks for entire period, let us use top 200 stocks by market cap, updated at every rebalance date. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "### Context \n",
    "We are trying to create a robot to replace a portfolio manger. Or to at least save time of a human portfolio manager by automating strategy creation as much as possible. \n",
    "\n",
    "Typically, a long-short equity portfolio manager would identify stocks, using a combination of fundamental and technical analysis, that she believes would out-perform the benchmark (typically an index) and those that would under-perform the index in the long-term (typically 2-3 years). \n",
    "\n",
    "She would go long the former and short the latter to create delta-neutral long-short portfolio of stocks. She would periodically, typically every month or on special events like earnings releases, re-assess his portfolio in light of fresh information, and possibly \"rebalance\" i.e. change the weights of stock holdings in her portfolio slightly or at times dramatically. \n",
    "\n",
    "Her objective is to achieve great long-term returns. The clients would choose to invest in those funds whose fund-managers have a track-record of consistently providing great returns on their investment after taking away all costs and fees. Clients are usually also concerned about risk of price fluctuation in the short-term, and not just long-term returns. \n",
    "\n",
    "### Formulation\n",
    "\n",
    "Lets say we are sitting at time $t$. We build our stock universe by using latest available market-cap information and choose top $N$ (say 200) stocks where both fundamental and price data are available. \n",
    "\n",
    "As of time $t$, we have information about company financials and stock price history for each of $N$ stocks in the form of a feature vector. These features are derived from the most recent earnings data released by each company. We will add more features that are \"technical\" i.e. not related to company's financial health, but only related to its stock price movements. Lets say there are $F$ features. All features are stored in matrix $X_t$ of size $NxF$. Let's say we have a model, yet to be optimized, which ranks the $N$ stocks as a function of all their features from \"best\" to \"worst\". We go long (i.e. buy) the top 20% stocks, and short (i.e. borrow and sell) the bottom 20% stocks making an equally weighted long short portfolio. \n",
    "\n",
    "We move to time $t+1$, lets say, the time is measured in months (typical rebalance horizon for long-term fund). Let the stock price of $i$th stock see a return of $y_{t,i}$ from time $t$ to $t+1$. At $t+1$, we repeat the process by a fresh ranking of refreshed top $N$ market-cap stocks as a function of $X_{t+1}$ and then build another quintile portfolio based on it. We will *rebalance* our portfolio by buying and selling stocks such that at $t+1$, we hold the new top 20% as long and new bottom 20% as short. \n",
    "\n",
    "How does the model rank the $N$ stocks? By estimating $y_t$ at time $t$, which is unknown at time $t$ but known at time $t+1$. By using the benefit of hindsight, we can create a training set for our model that is aware of the values of $y_t$ at each time $t$. Various machine learning algorithms can then be applied predict $y_t$. We can use mean squared error in prediction of $y_t$ as the objective function to be minimized. \n",
    "\n",
    "To select the best model, we will not limit ourselves to using mean squared error in stock returns as a metric, but instead use out of sample performance of the quintile portfolio as our guiding light. Standard metrics of portfolio performance like Compounded Annual Rate of Returns (CAGR), Annualized Volatility (Vol), Sharpe Ratio, Information Ratio and Maximum Drawdown (MaxDD) etc will be computed along with a visual cumulative return chart to help us make a balanced decision about model selecton. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "plt.rcParams[\"figure.figsize\"] = [15.0,6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the fundamental set data or the feature set \n",
    "dfn = pd.read_pickle('fundamental_data_dfn.pkl')\n",
    "\n",
    "#load the price data\n",
    "prc = pd.read_csv('TC1\\\\TC1_20170717.csv', \n",
    "                 names = ['ticker','date','open','high','low','last','close','volume','valueTraded'],\n",
    "                 header = None, parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform Data selection as discussed in the \"Data Selection\" section above \n",
    "#And create target variable (forward returns) y as discussed in \"Problem Formulation\"\n",
    "\n",
    "from earnings_dates import download_earnings_dates\n",
    "from earnings_dates import guess_quarter_end_date\n",
    "\n",
    "def createTrainingAndTestSet(dfn,prc):\n",
    "    ''' Create a training and test set '''\n",
    "    trainStart = '2005-03-31';trainEnd = '2011-12-31';\n",
    "    testStart = '2012-01-01' ; testEnd = '2015-12-31';\n",
    "#    testEnd = '2017-07-31';\n",
    "\n",
    "    rebalDays = 30 #Days after which to rebalance\n",
    "    stockUniverseSize = 200 #top stocks by market cap will be chosen as our universe\n",
    "    earningsReleaseDelay = 63 #days from quarter end we assume results will be public \n",
    "                              #important as we dont have earnings release dates before 2011\n",
    "    horizon = 30   #days for which we are predicting the returns\n",
    "    max_frac_nulls_returns = .05 #maximum fraction of null in returns of one ticker, beyond\n",
    "                                #which we will drop the ticker from dataset\n",
    "    \n",
    "    #Get dates training and test\n",
    "    trainDateIndex = pd.date_range(start=trainStart, end=trainEnd, freq=str(rebalDays)+'D')\n",
    "    testDateIndex = pd.date_range(start=testStart, end=testEnd, freq=str(rebalDays)+'D')\n",
    "\n",
    "    #We checked that MCAP is the same in standlone and consolidated in 24,442 and different\n",
    "    #only in 60. \n",
    "    #Since consolidaed is null frequently, lets use standalone to define MCAP. \n",
    "    #dfn[(dfn.indicator == 'MCAP') & (~dfn.consolidated.isnull()) & ~(dfn.consolidated == dfn.standalone)].count()    \n",
    "\n",
    "    #Create a dataframe sorted by date and then by market cap of tickers\n",
    "    df_mcap = dfn[dfn.indicator == 'MCAP'].sort_values(['date','standalone'],ascending = (True,False))\n",
    "    df_mcap = df_mcap[['date','ticker','standalone']]\n",
    "    df_mcap.columns = ['QuarterEndDate','ticker','MCAP']\n",
    "\n",
    "    #remove certain \"crappy stocks\" whose MCAP has serious errors identified painfully\n",
    "    crappy_stocks = pd.read_csv('crappy_stocks.csv')\n",
    "    crappy_stocks = crappy_stocks.ticker.tolist()\n",
    "    df_mcap = df_mcap[~df_mcap.ticker.isin(crappy_stocks)]\n",
    "\n",
    "    #sort prc data as per requirement of merge_asof\n",
    "    prc = prc.sort_values(['date','ticker'])\n",
    "   \n",
    "    def createDFtopN(dateIndex,df_mcap,stockUniverseSize):\n",
    "        '''Creates a DataFrame with cartesian product of all dates in dateIndex \n",
    "        and top stockUniverseSize market cap tickers'''\n",
    "        all_tickers = df_mcap.ticker.unique()\n",
    "        #Create the cartesian (cross) product of all dates and all tickers\n",
    "        multiIndex = pd.MultiIndex.from_product([dateIndex,all_tickers],names = ['date','ticker'])\n",
    "        X = pd.DataFrame(0,index=multiIndex,columns = ['temp'])\n",
    "        X.reset_index(inplace=True)\n",
    "        \n",
    "        #In the cross product of all dates and all tickers, as of join the market caps\n",
    "        X = pd.merge_asof(left=X,right=df_mcap,left_on='date',right_on='QuarterEndDate',by='ticker')\n",
    "        #There will be many null MCAPs because many stocks listed later and couldn't \n",
    "        #be filled by as of join in the cartesian product. Delete the nulls\n",
    "        X = X.loc[X.MCAP.notnull(),['date','ticker','MCAP']]\n",
    "        #For each date sort the tickers by descending market cap\n",
    "        X = X.sort_values(['date','MCAP'],ascending = (True,False))\n",
    "        #Now retain top stockUniverseSize and delete the rest\n",
    "        X = X.groupby('date').apply(lambda x:x[0:stockUniverseSize])\n",
    "        #Clean-up\n",
    "        X = X.drop('date',1)\n",
    "        X = X.reset_index()\n",
    "        X = X.drop('level_1',1)\n",
    "\n",
    "        #Check for bad data. Note this is not percentage but fractional change!\n",
    "        X['pct_change_mcap'] = X.groupby('ticker')['MCAP'].pct_change()\n",
    "        #We drop wherever market-cap more than 10 times in a quarter \n",
    "        #checked visually, such a change is more likely an error than real\n",
    "        X = X[X.pct_change_mcap.abs() < 10]\n",
    "\n",
    "        #Now let us prepare y, the target\n",
    "        #use merge_asof with exact match allowed. So if price is found for that date, we take that, \n",
    "        #else we search backwards for the first available price. \n",
    "        y = pd.merge_asof(X[['date', 'ticker']],prc[['date', 'ticker', 'close']], on = 'date', by='ticker')\n",
    "        y = y.rename(columns = {'date':'entry_date','close':'entry_price'})\n",
    "        y['date'] = y.entry_date + pd.Timedelta(horizon, unit='D')\n",
    "        #use merge_asof with exact match allowed. So if price is found for that date, we take that, \n",
    "        #else we search backwards for the first available price. \n",
    "        y = pd.merge_asof(y,prc[['date', 'ticker', 'close']], on = 'date', by='ticker')\n",
    "        y = y.rename(columns = {'date':'exit_date','close':'exit_price'})\n",
    "        #y = y.sort_values(['ticker','date'])\n",
    "        #y['return_1m'] = y.groupby('ticker')['close'].pct_change()\n",
    "        y['return'] = (y.exit_price - y.entry_price)/y.entry_price\n",
    "        \n",
    "        #Drop those tickers who have more than 5% (max_frac_nulls_returns) nulls in returns\n",
    "        tickers_frac_nulls = y.groupby('ticker').apply(lambda x: np.sum(x.isnull())/len(x) )['return']\n",
    "        tickers_to_retain = tickers_frac_nulls[tickers_frac_nulls < max_frac_nulls_returns].index.tolist()\n",
    "        X = X[X.ticker.isin(tickers_to_retain)]\n",
    "        y = y[y.ticker.isin(tickers_to_retain)]\n",
    "        return X,y\n",
    "    \n",
    "    def add_features(X,dfn,earnings_dates,earningsReleaseDelay):\n",
    "\n",
    "        #Populate BoardMeetingDate by matching guessed QuarterEndDate in earnings_dates\n",
    "        #with date in dfn for each ticker\n",
    "        dfn = pd.merge(dfn,earnings_dates[['QuarterEndDate','BoardMeetingDate','ticker','Purpose']],\n",
    "                              left_on=['date','ticker'], right_on=['QuarterEndDate','ticker'])\n",
    "\n",
    "        #we know that many (certainly before 2010) earnings dates are missing\n",
    "        #so populate a new column with a default date created by adding a delay\n",
    "        dfn['EarningsReleaseDate'] = dfn['date'] + pd.Timedelta(earningsReleaseDelay, unit='D')\n",
    "\n",
    "        #Now overwrite this column with correct dates wherever available. \n",
    "        dfn.loc[dfn.BoardMeetingDate.notnull(),['EarningsReleaseDate']] = dfn.BoardMeetingDate[dfn.BoardMeetingDate.notnull()]\n",
    "\n",
    "        #rename indicator to feature \n",
    "        dfn = dfn.rename(columns={'indicator':'feature'})\n",
    "\n",
    "        #sort by earnings release date as to prepare of as-of-join\n",
    "        dfn = dfn.sort_values(['EarningsReleaseDate','ticker','feature'])\n",
    "\n",
    "        #create a place holder for all dates, tickers and features\n",
    "        all_features = dfn.feature.unique()\n",
    "        X['dummy'] = 0 #trick to effect cross product\n",
    "        df_all_features =  pd.DataFrame(all_features, columns = ['feature'])\n",
    "        df_all_features['dummy'] = 0 #trick to effect cross product\n",
    "        #gets the cartesian product\n",
    "        X_big = pd.merge(X,df_all_features,on='dummy')\n",
    "\n",
    "        #In the cross product of rebalnce dates, top 200 tickers, all features, \n",
    "        #as of join 'standalone' and 'consolidated' values\n",
    "        #We have chosesn allow_exact_matches=False because earnings releases are\n",
    "        #usually after market close. So the earliest we can act on that information is \n",
    "        #on the next day\n",
    "        X_big_filled = pd.merge_asof(left=X_big,right=dfn,left_on='date',\n",
    "                                     right_on='EarningsReleaseDate',by=['ticker','feature'],\n",
    "                                    allow_exact_matches=False)\n",
    "\n",
    "        #remove null rows and useless columns \n",
    "        X_big_filled = X_big_filled.loc[X_big_filled.freq.notnull(),['date_x','EarningsReleaseDate','QuarterEndDate','ticker','feature','standalone','consolidated']]\n",
    "        X_big_filled = X_big_filled.rename(columns={'date_x':'date'})\n",
    "        return X_big_filled\n",
    "\n",
    "    #Create train and test dataframes of rows rebalDates x stockUniverseSize\n",
    "    X_train,y_train = createDFtopN(trainDateIndex,df_mcap,stockUniverseSize)\n",
    "    X_test,y_test = createDFtopN(testDateIndex,df_mcap,stockUniverseSize)\n",
    "\n",
    "    #Get earnings release dates (downloaded from NSE for 2011 onwards for 1000+ stocks) \n",
    "    earnings_dates = download_earnings_dates('sending any input') \n",
    "    earnings_dates = guess_quarter_end_date(earnings_dates)\n",
    "    \n",
    "    #Create rebalance dates x top stockUniverseSize x available features and their values\n",
    "    #(earnings release dates are required to avoid forward looking)\n",
    "    X_train = add_features(X_train,dfn,earnings_dates,earningsReleaseDelay)\n",
    "    X_test = add_features(X_test,dfn,earnings_dates,earningsReleaseDelay)\n",
    "    \n",
    "    return X_train,y_train, X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earnings_release_dates.csv found, loading from it.\n",
      "If you want to download from NSE, delete this file\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Call the function defined above to get training and test sets\n",
    "X_train,y_train, X_test,y_test = createTrainingAndTestSet(dfn,prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Investment signal creation and classification: \n",
    "A close look at fundamental data set to see whether it needs enrichment i.e. computing well known indicators to make a comprehensive list of possible investment signals classified into various factors like growth, value, quality, size, momentum etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This description was available in the original data source \n",
    "solvency = pd.read_excel('DEB_Fundamental_Indicators.xlsx',sheetname='Solvency and Fundamentals')\n",
    "valuation = pd.read_excel('DEB_Fundamental_Indicators.xlsx',sheetname='Risk, Pricing and Valuation')\n",
    "profitability = pd.read_excel('DEB_Fundamental_Indicators.xlsx',sheetname='Profitability and Management')\n",
    "earnings_growth = pd.read_excel('DEB_Fundamental_Indicators.xlsx',sheetname='Income Statement')\n",
    "cashflow_growth = pd.read_excel('DEB_Fundamental_Indicators.xlsx',sheetname='Cash Flow Statement')\n",
    "balance_sheet = pd.read_excel('DEB_Fundamental_Indicators.xlsx',sheetname='Balance Sheet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvency['category'] = 'solvency'\n",
    "valuation['category'] = 'valuation'\n",
    "profitability['category'] = 'profitability'\n",
    "earnings_growth['category'] = 'earnings_growth'\n",
    "cashflow_growth['category'] = 'cashflow_growth'\n",
    "balance_sheet['category'] = 'balance_sheet'\n",
    "feature_desription = pd.concat([solvency,valuation,profitability,earnings_growth,\n",
    "                                cashflow_growth,balance_sheet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>feature</th>\n",
       "      <th>name</th>\n",
       "      <th>freq</th>\n",
       "      <th>priorImportance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balance_sheet</td>\n",
       "      <td>BVSH3</td>\n",
       "      <td>3 Year CAGR Growth in Book Value per Share</td>\n",
       "      <td>Annual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>balance_sheet</td>\n",
       "      <td>BVSH5</td>\n",
       "      <td>5 Year CAGR Growth in Book Value per Share</td>\n",
       "      <td>Annual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cashflow_growth</td>\n",
       "      <td>CEPS1</td>\n",
       "      <td>1 Year CAGR Growth in Cash EPS per Share</td>\n",
       "      <td>Annual</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cashflow_growth</td>\n",
       "      <td>CEPS3</td>\n",
       "      <td>3 Year CAGR Growth in Cash EPS per Share</td>\n",
       "      <td>Annual</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earnings_growth</td>\n",
       "      <td>EPS4Q</td>\n",
       "      <td>4 Quarter Growth in EPS</td>\n",
       "      <td>Quarterly</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>earnings_growth</td>\n",
       "      <td>EPS8Q</td>\n",
       "      <td>8 Quarter Growth in EPS</td>\n",
       "      <td>Quarterly</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>profitability</td>\n",
       "      <td>WCTO</td>\n",
       "      <td>Working Capital Turnover</td>\n",
       "      <td>Annual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>profitability</td>\n",
       "      <td>NBTO</td>\n",
       "      <td>Fixed Asset Turnover</td>\n",
       "      <td>Annual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>solvency</td>\n",
       "      <td>DEBT_CE</td>\n",
       "      <td>Debt To Capital</td>\n",
       "      <td>Annual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>solvency</td>\n",
       "      <td>AE</td>\n",
       "      <td>Assets to Shareholder Equity</td>\n",
       "      <td>Annual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>valuation</td>\n",
       "      <td>EV_NP</td>\n",
       "      <td>EV to Earnings</td>\n",
       "      <td>Annual</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>valuation</td>\n",
       "      <td>EV_ASSETS</td>\n",
       "      <td>EV to Assets</td>\n",
       "      <td>Annual</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category    feature                                        name  \\\n",
       "0     balance_sheet      BVSH3  3 Year CAGR Growth in Book Value per Share   \n",
       "1     balance_sheet      BVSH5  5 Year CAGR Growth in Book Value per Share   \n",
       "2   cashflow_growth      CEPS1    1 Year CAGR Growth in Cash EPS per Share   \n",
       "3   cashflow_growth      CEPS3    3 Year CAGR Growth in Cash EPS per Share   \n",
       "4   earnings_growth      EPS4Q                     4 Quarter Growth in EPS   \n",
       "5   earnings_growth      EPS8Q                     8 Quarter Growth in EPS   \n",
       "6     profitability       WCTO                    Working Capital Turnover   \n",
       "7     profitability       NBTO                        Fixed Asset Turnover   \n",
       "8          solvency    DEBT_CE                             Debt To Capital   \n",
       "9          solvency         AE                Assets to Shareholder Equity   \n",
       "10        valuation      EV_NP                              EV to Earnings   \n",
       "11        valuation  EV_ASSETS                                EV to Assets   \n",
       "\n",
       "         freq  priorImportance  \n",
       "0      Annual                1  \n",
       "1      Annual                1  \n",
       "2      Annual                2  \n",
       "3      Annual                2  \n",
       "4   Quarterly                2  \n",
       "5   Quarterly                2  \n",
       "6      Annual                1  \n",
       "7      Annual                1  \n",
       "8      Annual                1  \n",
       "9      Annual                1  \n",
       "10     Annual                2  \n",
       "11     Annual                2  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show 2 features from each category\n",
    "feature_desription.groupby('category').apply(lambda x:x[-3:-1]).drop('category',1).reset_index().drop('level_1',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#priorImportance is a column I have added by using my knowledge of finance\n",
    "#priorImportance = 0: value not directly related to growth in stock price\n",
    "#priorImportance = 1: not the best but value potentially related to growth in stock price\n",
    "#priorImportance = 0: value directly related to growth in stock price, commonly used as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
