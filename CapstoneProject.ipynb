{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Capstone Project: Systematic Trading Strategies\n",
    "\n",
    "## Goal: \n",
    "Implement a process of developing trading strategies in Indian Equity Markets using fundamental data of liquid listed companies that maximises out of sample average returns keeping risk under control.\n",
    "\n",
    "## Who cares?\n",
    "\n",
    "I do. I am planning to start my own investment firm. I have not focussed much on fundamental data before, neither have I explored advanced machine learning techniques to develop trading strategies. I specialize in analyzing short-term (seconds to minutes) movements in markets, not over months and years. If there indeed is value derived out of using some subset of Data Science that I learn during this course, I am happy to invest my own money on these trading strategies. Even otherwise, I believe making investment process data-driven and automated has benefits for everyone interested invest wisely and with minimum cost. \n",
    "\n",
    "## Data\n",
    "\n",
    "I have subscribed to two data sets from Quandl.\n",
    "\n",
    "1. [https://www.quandl.com/data/DEB-Core-India-Fundamentals-Data](https://www.quandl.com/data/DEB-Core-India-Fundamentals-Data) Contains fundamental indicators derived from company financials from published balance sheet, income statements and cash flow statements from roughly 4000 listed stocks on NSE or BSE.  History goes back to 2005\n",
    "2. [https://www.quandl.com/data/TC1-Indian-Equities-Adjusted-End-of-Day-Prices](https://www.quandl.com/data/TC1-Indian-Equities-Adjusted-End-of-Day-Prices) Contains daily prices (open, low, high, close), volume and value traded for constituents of top 500 NSE listed stocks by market capitalization. The prices are adjusted from corporate actions like stock splits, rights issues, dividends, buy backs etc.\n",
    "\n",
    "## Approach\n",
    "\n",
    "We wish to follow the approach of a white paper published by a Deutsche Bank research group. In that paper the authors review various machine learning algorithms and apply them in practice on Japanese Equity markets data. The Following are the steps involved\n",
    "\n",
    "1. **Data Selection** To decide the stock universe, date range for training set and test-set. Factors involved:\n",
    "  1. Data availability\n",
    "  2. Stock liquidity\n",
    "  3. Avoid any bias in universe selection like survivorship\n",
    "2. ** Problem Formulation: ** Describe \n",
    "    1. the context,\n",
    "    2. the feature space, \n",
    "    3. the target variable, \n",
    "    3. the optimization problem and \n",
    "    4. the model selection metric and \n",
    "    5. the evaluation criteria\n",
    "2. **Investment signal creation and classification**: A close look at fundamental data set to see whether it needs enrichment i.e. computing well known indicators to make a comprehensive list of possible investment signals classified into various factors like growth, value, quality, size, momentum etc\n",
    "3. **Visualize performance of individual investment signals ** If each investment signal is used only by itself as a ranker for long stock portfolio, how is the in-sample performance of the strategy.\n",
    "4. **Data pre-processing** like filling missing data, normalizing, uniformizing, sector-neutralizing, quantizing, winsorizing etc to make variations in output variable (returns) more sensitive to variations in input variables and also make combining variables more sensible.\n",
    "5. **Create a benchmark** by using a simple linear model (with some basic checks) for other machine learning techniques to see if they do any better\n",
    "6. **Test most promising models** as per the paper and see if they perform any better than the simple model\n",
    "7. **Document the conclusions and insights**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection \n",
    "To decide the date range for training set and test-set and stock universe for this study. Factors involved:\n",
    "\n",
    "1. Data availability: \n",
    "    1. Fundamental Data-set is available 2005 onwards. \n",
    "    2. Adjusted Price data is available 2001 onwards. \n",
    "    3. Earnings release dates are available 2011 onwards. \n",
    "    4. There are more than 1000 stocks common in all the three data-sets\n",
    "    5. ** Decisions: ** \n",
    "        1. Lets *not* throw away 2005-10 for absence of earnings release dates. Instead use it with a delay of 63 days from quarter end whenever earnings release dates are not available\n",
    "        2. Training set: 2005 to 2011 \n",
    "        3. Test set: 2012 - 2015 \n",
    "        4. Reserved set (only for final reporting): 2016 - 2017 \n",
    "2. Stock liquidity:\n",
    "    1. Small cap stocks are known to be prone to manipulation (both of stock market and company fundamentals) \n",
    "    2. Trading in large amounts in smaller stocks will lead to heavy transaction costs. \n",
    "    3. ** Decision: ** Let us use top 200 stocks by market cap at each rebalance date. \n",
    "3. Avoid any bias in universe selection like survivorship:\n",
    "    1. ** Decision: ** Instead of a fixed universe of stocks for entire period, let us use top 200 stocks by market cap, updated at every rebalance date. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#fundamental data or the feature set\n",
    "dfn = pd.read_pickle('fundamental_data_dfn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatureSet(dfn):\n",
    "    ''' Create a training and test set '''\n",
    "    trainStart = '2005-03-31';trainEnd = '2011-12-31';\n",
    "    testStart = '2012-01-01' ; testEnd = '2015-12-31';\n",
    "#    testEnd = '2017-07-31';\n",
    "\n",
    "    rebalDays = 30 #Days after which to rebalance\n",
    "    stockUniverseSize = 200 #top stocks by market cap will be chosen as our universe\n",
    "    earningsReleaseDelay = 63 #days from quarter end we assume results will be public \n",
    "                              #important as we dont have earnings release dates before 2011\n",
    "    \n",
    "    #Get dates training and test\n",
    "    trainDateIndex = pd.date_range(start=trainStart, end=trainEnd, freq=str(rebalDays)+'D')\n",
    "    testDateIndex = pd.date_range(start=testStart, end=testEnd, freq=str(rebalDays)+'D')\n",
    "\n",
    "    #We checked that MCAP is the same in standlone and consolidated in 24,442 and different only in 60. \n",
    "    #Since consolidaed is null frequently, lets use standalone to define MCAP. \n",
    "    #dfn[(dfn.indicator == 'MCAP') & (~dfn.consolidated.isnull()) & ~(dfn.consolidated == dfn.standalone)].count()    \n",
    "\n",
    "    #Create a dataframe sorted by date and then by market cap of tickers\n",
    "    df_mcap = dfn[dfn.indicator == 'MCAP'].sort_values(['date','standalone'],ascending = (True,False))\n",
    "    df_mcap = df_mcap[['date','ticker','standalone']]\n",
    "    df_mcap.columns = ['QuarterEndDate','ticker','MCAP']\n",
    "\n",
    "    #remove certain \"crappy stocks\" whose MCAP has serious errors identified painfully\n",
    "    crappy_stocks = pd.read_csv('crappy_stocks.csv')\n",
    "    crappy_stocks = crappy_stocks.ticker.tolist()\n",
    "    df_mcap = df_mcap[~df_mcap.ticker.isin(crappy_stocks)]\n",
    "    \n",
    "    def createDFtopN(dateIndex,df_mcap,stockUniverseSize):\n",
    "        '''Creates a DataFrame with cartesian product of all dates in dateIndex \n",
    "        and top stockUniverseSize market cap tickers'''\n",
    "        all_tickers = df_mcap.ticker.unique()\n",
    "        #Create the cartesian (cross) product of all dates and all tickers\n",
    "        multiIndex = pd.MultiIndex.from_product([dateIndex,all_tickers],names = ['date','ticker'])\n",
    "        X = pd.DataFrame(0,index=multiIndex,columns = ['temp'])\n",
    "        X.reset_index(inplace=True)\n",
    "        \n",
    "        #In the cross product of all dates and all tickers, as of join the market caps\n",
    "        X = pd.merge_asof(left=X,right=df_mcap,left_on='date',right_on='QuarterEndDate',by='ticker')\n",
    "        #There will be many null MCAPs because many stocks listed later and couldn't \n",
    "        #be filled by as of join in the cartesian product. Delete the nulls\n",
    "        X = X.loc[X.MCAP.notnull(),['date','ticker','MCAP']]\n",
    "        #For each date sort the tickers by descending market cap\n",
    "        X = X.sort_values(['date','MCAP'],ascending = (True,False))\n",
    "        #Now retain top stockUniverseSize and delete the rest\n",
    "        X = X.groupby('date').apply(lambda x:x[0:stockUniverseSize])\n",
    "        #Clean-up\n",
    "        X = X.drop('date',1)\n",
    "        X = X.reset_index()\n",
    "        X = X.drop('level_1',1)\n",
    "\n",
    "        #Check for bad data. Note this is not percentage but fractional change!\n",
    "        X['pct_change_mcap'] = X.groupby('ticker')['MCAP'].pct_change()\n",
    "        #We drop wherever market-cap more than 10 times in a quarter \n",
    "        #checked visually, such a change is more likely an error than real\n",
    "        X = X[X.pct_change_mcap.abs() < 10]\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def add_features(X,dfn,earningsReleaseDelay):\n",
    "        #in dfn, add EarningsReleaseDate and rename indicator to feature\n",
    "        dfn['EarningsReleaseDate'] = dfn['date'] + pd.Timedelta(earningsReleaseDelay, unit='D')\n",
    "        dfn = dfn.rename(columns={'indicator':'feature'})\n",
    "        dfn = dfn.sort_values(['EarningsReleaseDate','ticker','feature'])\n",
    "\n",
    "        #create a place holder for all dates, tickers and features\n",
    "        all_features = dfn.feature.unique()\n",
    "        X['dummy'] = 0 #trick to effect cross product\n",
    "        df_all_features =  pd.DataFrame(all_features, columns = ['feature'])\n",
    "        df_all_features['dummy'] = 0 #trick to effect cross product\n",
    "        #gets the cartesian product\n",
    "        X_big = pd.merge(X,df_all_features,on='dummy')\n",
    "\n",
    "        #In the cross product of rebalnce dates, top 200 tickers, all features, \n",
    "        #as of join 'standalone' and 'consolidated' values\n",
    "        X_big_filled = pd.merge_asof(left=X_big,right=dfn,left_on='date',right_on='EarningsReleaseDate',by=['ticker','feature'])\n",
    "\n",
    "        #remove null rows and useless columns \n",
    "        X_big_filled = X_big_filled.loc[X_big_filled.freq.notnull(),['date_x','EarningsReleaseDate','ticker','feature','standalone','consolidated']]\n",
    "        X_big_filled = X_big_filled.rename(columns={'date_x':'date'})\n",
    "        return X_big_filled\n",
    "\n",
    "    #Create rebalance dates x top stockUniverseSize tickers train and test place holders\n",
    "    X_train = createDFtopN(trainDateIndex,df_mcap,stockUniverseSize)\n",
    "    X_test = createDFtopN(testDateIndex,df_mcap,stockUniverseSize)\n",
    "\n",
    "    #Create rebalance dates x top stockUniverseSize x available features and their values\n",
    "    X_train = add_features(X_train,dfn,earningsReleaseDelay)\n",
    "    X_test = add_features(X_test,dfn,earningsReleaseDelay)\n",
    "    \n",
    "    return X_train, X_test\n",
    "X_train, X_test = createFeatureSet(dfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "### Context \n",
    "We are trying to create a robot to replace a portfolio manger. Typically, a long-short equity portfolio manager would identify stocks, using a combination of fundamental and technical analysis, that she believes would out-perform the benchmark (typically an index) and those that would under-perform the index in the long-term (typically 2-3 years). \n",
    "\n",
    "She would go long the former and short the latter to create delta-neutral long-short portfolio of stocks. She would periodically, typically every month or on special events like earnings releases, re-assess his portfolio in light of fresh information, and possibly \"rebalance\" i.e. change the weights of stock holdings in her portfolio slightly or at times dramatically. \n",
    "\n",
    "Her objective is to achieve great long-term returns. The clients would choose to invest in those funds whose fund-managers have a track-record of consistently providing great returns on their investment after taking away all costs and fees. Clients are usually also concerned about risk of price fluctuation in the short-term, and not just long-term returns. \n",
    "\n",
    "### Formulation\n",
    "\n",
    "Lets say we are sitting at time $t$. We build our stock universe by using latest available market-cap information and choose top $N$ (say 200) stocks where both fundamental and price data are available. \n",
    "\n",
    "As of time $t$, we have information about company financials and stock price history for each of $N$ stocks in the form of a feature vector. These features are derived from the most recent earnings data released by each company. We will add more features that are \"technical\" i.e. not related to company's financial health, but only related to its stock price movements. Lets say there are $F$ features. All features are stored in matrix $X_t$ of size $NxF$. Let's say we have a model, yet to be optimized, which ranks the $N$ stocks as a function of all their features from \"best\" to \"worst\". We go long (i.e. buy) the top 20% stocks, and short (i.e. borrow and sell) the bottom 20% stocks making an equally weighted long short portfolio. \n",
    "\n",
    "We move to time $t+1$, lets say, the time is measured in months (typical rebalance horizon for long-term fund). Let the stock price of $i$th stock see a return of $y_{t,i}$ from time $t$ to $t+1$. At $t+1$, we repeat the process by a fresh ranking of refreshed top $N$ market-cap stocks as a function of $X_{t+1}$ and then build another quintile portfolio based on it. We will *rebalance* our portfolio by buying and selling stocks such that at $t+1$, we hold the new top 20% as long and new bottom 20% as short. \n",
    "\n",
    "How does the model rank the $N$ stocks? By estimating $y_t$ at time $t$, which is unknown at time $t$ but known at time $t+1$. By using the benefit of hindsight, we can create a training set for our model that is aware of the values of $y_t$ at each time $t$. Various machine learning algorithms can then be applied predict $y_t$. We can use mean squared error in prediction of $y_t$ as the objective function to be minimized. \n",
    "\n",
    "To select the best model, we will not limit ourselves to using mean squared error in stock returns as a metric, but instead use out of sample performance of the quintile portfolio as our guiding light. Standard metrics of portfolio performance like Compounded Annual Rate of Returns (CAGR), Annualized Volatility (Vol), Sharpe Ratio, Information Ratio and Maximum Drawdown (MaxDD) etc will be computed along with a visual cumulative return chart to help us make a balanced decision about model selecton. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
